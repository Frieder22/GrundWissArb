% definition sattistisches Modell
@inbook{Cox_2006,
place={Cambridge},
title={Randomization-based analysis},
booktitle={Principles of Statistical Inference},
publisher={Cambridge University Press},
author={Cox, D. R.}, year={2006}, pages={178–193}
}

% MSE
@book{MSE,
  author    = {Casella, George and Berger, Roger L.},
  title     = {Statistical Inference},
  edition   = {2},
  publisher = {Duxbury Press},
  year      = {2002}
}

% methode der kleinsten quadrate
@book{LMSE,
  author    = {Draper, Norman R. and Smith, Harry},
  title     = {Applied Regression Analysis},
  edition   = {3},
  publisher = {Wiley},
  year      = {1998}
}

% Optimierungsverfahren
@book{Hollstein2023,
  author    = {Hollstein, Ralf},
  title     = {Optimierungsmethoden: Einführung in klassische, naturanaloge und neuronale Optimierungen},
  publisher = {Springer Fachmedien Wiesbaden},
  year      = {2023},
  isbn      = {978-3-658-39854-5}
}

% pattern recognition
@book{paternRec,
  author    = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
  title     = {Pattern Classification},
  edition   = {2},
  publisher = {Wiley},
  year      = {2001}
}

% für eindeutigkeit von polynomen
@Inbook{Schwarz2011,
author="Schwarz, Hans Rudolf
and K{\"o}ckler, Norbert",
title="Interpolation und Approximation",
bookTitle="Numerische Mathematik",
year="2011",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="91--182",
abstract="Dieses Kapitel soll die wichtigsten M{\"o}glichkeiten beschreiben, mit denen eine reellwertige Funktion f (x) der reellen Variablen x oder eine vorliegende Datentabelle (xi, yi) durch eine einfache Funktion -- sagen wir g(x) -- in geschlossener Form angen{\"a}hert werden k{\"o}nnen.",
isbn="978-3-8348-8166-3",
doi="10.1007/978-3-8348-8166-3_4",
url="https://doi.org/10.1007/978-3-8348-8166-3_4"
}

% BEschreibung over- und under-fitting
% Plot variance/Bias
@Inbook{Hastie2009,
	author="Hastie, Trevor
	and Tibshirani, Robert
	and Friedman, Jerome",
	title="Model Assessment and Selection",
	bookTitle="The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
	year="2009",
	publisher="Springer New York",
	address="New York, NY",
	pages="219--259",
	isbn="978-0-387-84858-7",
	doi="10.1007/978-0-387-84858-7_7",
	url="https://doi.org/10.1007/978-0-387-84858-7_7"
}

%for Lasso and Ridge Regularization
@Inbook{Hastie2009chapter3,
	author="Hastie, Trevor
	and Tibshirani, Robert
	and Friedman, Jerome",
	title="Linear Methods for Regression",
	bookTitle="The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
	year="2009",
	publisher="Springer New York",
	address="New York, NY",
	pages="43--99",
	isbn="978-0-387-84858-7",
	doi="10.1007/978-0-387-84858-7_3",
	url="https://doi.org/10.1007/978-0-387-84858-7_3"
}

% more data is better argument
@misc{moreData,
	title={More is Better in Modern Machine Learning: when Infinite Overparameterization is Optimal and Overfitting is Obligatory}, 
	author={James B. Simon and Dhruva Karkada and Nikhil Ghosh and Mikhail Belkin},
	year={2024},
	eprint={2311.14646},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2311.14646}, 
}

% quelle für data augmentation
@article{dataAugmentation,
	title = {Data augmentation: A comprehensive survey of modern approaches},
	journal = {Array},
	volume = {16},
	pages = {100258},
	year = {2022},
	issn = {2590-0056},
	doi = {https://doi.org/10.1016/j.array.2022.100258},
	url = {https://www.sciencedirect.com/science/article/pii/S2590005622000911},
	author = {Alhassan Mumuni and Fuseini Mumuni},
	keywords = {Review of data augmentation, Computer vision, Generative adversarial network, Meta-learning, Synthetic data, Machine learning},
	abstract = {To ensure good performance, modern machine learning models typically require large amounts of quality annotated data. Meanwhile, the data collection and annotation processes are usually performed manually, and consume a lot of time and resources. The quality and representativeness of curated data for a given task is usually dictated by the natural availability of clean data in the particular domain as well as the level of expertise of developers involved. In many real-world application settings it is often not feasible to obtain sufficient training data. Currently, data augmentation is the most effective way of alleviating this problem. The main goal of data augmentation is to increase the volume, quality and diversity of training data. This paper presents an extensive and thorough review of data augmentation methods applicable in computer vision domains. The focus is on more recent and advanced data augmentation techniques. The surveyed methods include deeply learned augmentation strategies as well as feature-level and meta-learning-based data augmentation techniques. Data synthesis approaches based on realistic 3D graphics modeling, neural rendering, and generative adversarial networks are also covered. Different from previous surveys, we cover a more extensive array of modern techniques and applications. We also compare the performance of several state-of-the-art augmentation methods and present a rigorous discussion of the effectiveness of various techniques in different scenarios of use based on performance results on different datasets and tasks.}
}

% Textbook only using variance Bias tradeoff
@Inbook{Wang2025,
	author="Wang, Wenmin",
	title="Supervised Learning Paradigm",
	bookTitle="Principles of Machine Learning: The Three Perspectives",
	year="2025",
	publisher="Springer Nature Singapore",
	address="Singapore",
	pages="269--290",
	abstract="Supervised learning is one of the three major paradigms of machine learning. This chapter begins from the definition of supervised learning and explains its working principle using formal and illustrated descriptions. Second, we introduce the classic tasks of supervised learning. Next, we respectively discuss the two important factors to guarantee the performance for a supervised learning algorithm: one is the bias-variance problems including bias-variance trade-off and bias-variance decomposition, and another one is the risk minimization principles, namely expected risk minimization, empirical risk minimization, and structural risk minimization. We then introduce several variants of supervised learning, such as weakly supervised learning, semi-supervised learning, and label-free supervision, where the label-free supervision is a supervised learning approach based on domain knowledge without manual labeling. Finally, we introduce the no free lunch theorems, which are related to machine learning.",
	isbn="978-981-97-5333-8",
	doi="10.1007/978-981-97-5333-8_8",
	url="https://doi.org/10.1007/978-981-97-5333-8_8"
}
